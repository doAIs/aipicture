"""
è¿›é˜¶ç¤ºä¾‹: æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼ˆå¸¦å‚æ•°æ§åˆ¶ï¼‰
åŒ…å«æ›´å¤šå‚æ•°é€‰é¡¹ï¼Œå¯ä»¥ç²¾ç»†æ§åˆ¶ç”Ÿæˆæ•ˆæœ
"""

from diffusers import DiffusionPipeline
import torch
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils.modules_utils import save_video, get_device, set_seed, load_model_from_local_file, load_model_with_fallback
from config.modules_config import DEFAULT_VIDEO_STEPS, DEFAULT_VIDEO_FRAMES, DEFAULT_VIDEO_FPS, DEFAULT_VIDEO_HEIGHT, DEFAULT_VIDEO_WIDTH, DEFAULT_SEED, LOCAL_VIDEO_MODEL_PATH


class AdvancedTextToVideo:
    """é«˜çº§æ–‡æœ¬ç”Ÿæˆè§†é¢‘ç±»"""
    
    def __init__(self, model_name: str = "damo-vilab/text-to-video-ms-1.7b", local_model_path: str = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_name: åœ¨çº¿æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼‰
            local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
                            - å¦‚æœä¸º Noneï¼Œåˆ™ä»é…ç½®æ–‡ä»¶ config.LOCAL_VIDEO_MODEL_PATH è¯»å–
                            - å¦‚æœä¸º "" æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                            - å¦‚æœæŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
        """
        self.device = get_device()
        self.model_name = model_name
        
        # ç¡®å®šæœ¬åœ°æ¨¡å‹è·¯å¾„çš„ä¼˜å…ˆçº§ï¼š
        # 1. å¦‚æœä¼ å…¥äº† local_model_path å‚æ•°ï¼Œä½¿ç”¨ä¼ å…¥çš„å€¼ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºç¦ç”¨ï¼‰
        # 2. å¦åˆ™ä»é…ç½®æ–‡ä»¶è¯»å– LOCAL_VIDEO_MODEL_PATH
        if local_model_path is not None:
            self.local_model_path = local_model_path if local_model_path else None
        else:
            # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœé…ç½®ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æœ¬åœ°æ¨¡å‹
            self.local_model_path = LOCAL_VIDEO_MODEL_PATH if LOCAL_VIDEO_MODEL_PATH else None
        
        self.pipe = None
        print(f"åˆå§‹åŒ–è§†é¢‘ç”Ÿæˆå™¨ï¼Œåœ¨çº¿æ¨¡å‹: {model_name}")
        if self.local_model_path:
            print(f"æœ¬åœ°æ¨¡å‹è·¯å¾„: {self.local_model_path}")
        else:
            print("æœ¬åœ°æ¨¡å‹: å·²ç¦ç”¨ï¼ˆä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼‰")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œåªåœ¨éœ€è¦æ—¶åŠ è½½ï¼‰
        ä¼˜å…ˆåŠ è½½æœ¬åœ°ç¦»çº¿æ¨¡å‹ï¼Œå¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨åˆ™åŠ è½½åœ¨çº¿æ¨¡å‹
        """
        if self.pipe is None:
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            if self.device == "cuda":
                torch_dtype = torch.float16
            else:
                torch_dtype = torch.float32
            
            model_kwargs = {
                "torch_dtype": torch_dtype,
            }
            
            # ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœå·²é…ç½®ï¼‰
            if self.local_model_path and os.path.exists(self.local_model_path):
                print(f"\nâœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {self.local_model_path}")
                print("   ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¦»çº¿æ¨¡å‹...")
                try:
                    self.pipe = load_model_from_local_file(
                        DiffusionPipeline,
                        self.local_model_path,
                        **model_kwargs
                    )
                    self.pipe = self.pipe.to(self.device)
                    print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                except Exception as e:
                    print(f"âš ï¸  æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    print(f"   å›é€€åˆ°åœ¨çº¿æ¨¡å‹: {self.model_name}")
                    # æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åœ¨çº¿æ¨¡å‹
                    self.pipe = load_model_with_fallback(
                        DiffusionPipeline,
                        self.model_name,
                        **model_kwargs
                    )
                    self.pipe = self.pipe.to(self.device)
            elif self.local_model_path:
                # æœ¬åœ°æ¨¡å‹è·¯å¾„å·²é…ç½®ä½†ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                print(f"\nâš ï¸  æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.local_model_path}")
                print(f"   ä½¿ç”¨åœ¨çº¿æ¨¡å‹: {self.model_name}")
                print("   æ³¨æ„ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹è¾ƒå¤§ï¼Œä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
                self.pipe = load_model_with_fallback(
                    DiffusionPipeline,
                    self.model_name,
                    **model_kwargs
                )
                self.pipe = self.pipe.to(self.device)
            else:
                # æœ¬åœ°æ¨¡å‹æœªé…ç½®ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                print(f"\nğŸ“¡ ä½¿ç”¨åœ¨çº¿æ¨¡å‹: {self.model_name}")
                print("   æ³¨æ„ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹è¾ƒå¤§ï¼Œä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
                self.pipe = load_model_with_fallback(
                    DiffusionPipeline,
                    self.model_name,
                    **model_kwargs
                )
                self.pipe = self.pipe.to(self.device)
            
            # ä¼˜åŒ–ï¼šå¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
            try:
                self.pipe.enable_attention_slicing()
                print("å·²å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ï¼ˆèŠ‚çœå†…å­˜ï¼‰")
            except:
                pass
            
            print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def generate(
        self,
        prompt: str,
        negative_prompt: str = None,
        num_inference_steps: int = DEFAULT_VIDEO_STEPS,
        num_frames: int = DEFAULT_VIDEO_FRAMES,
        fps: int = DEFAULT_VIDEO_FPS,
        height: int = DEFAULT_VIDEO_HEIGHT,
        width: int = DEFAULT_VIDEO_WIDTH,
        seed: int = DEFAULT_SEED,
        output_name: str = None
    ):
        """
        ç”Ÿæˆè§†é¢‘
        
        Args:
            prompt: æ­£é¢æç¤ºè¯ï¼ˆæè¿°æƒ³è¦çš„å†…å®¹ï¼‰
            negative_prompt: è´Ÿé¢æç¤ºè¯ï¼ˆæè¿°ä¸æƒ³è¦çš„å†…å®¹ï¼‰
            num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆ20-100ï¼Œè¶Šå¤šè´¨é‡è¶Šå¥½ä½†è¶Šæ…¢ï¼‰
            num_frames: è§†é¢‘å¸§æ•°ï¼ˆ8-32ï¼Œè¶Šå¤šè¶Šæµç•…ä½†è¶Šæ…¢ï¼‰
            fps: å¸§ç‡ï¼ˆ4-12ï¼Œå½±å“æ’­æ”¾é€Ÿåº¦ï¼‰
            height: è§†é¢‘é«˜åº¦ï¼ˆå¿…é¡»æ˜¯8çš„å€æ•°ï¼‰
            width: è§†é¢‘å®½åº¦ï¼ˆå¿…é¡»æ˜¯8çš„å€æ•°ï¼‰
            seed: éšæœºç§å­ï¼ˆç”¨äºå¤ç°ç»“æœï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶å
        
        Returns:
            ç”Ÿæˆçš„è§†é¢‘å¸§å’Œæ–‡ä»¶è·¯å¾„
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self.load_model()
        
        # è®¾ç½®éšæœºç§å­
        if seed is not None:
            set_seed(seed)
        
        print(f"\n{'='*60}")
        print(f"ç”Ÿæˆå‚æ•°:")
        print(f"  æç¤ºè¯: {prompt}")
        if negative_prompt:
            print(f"  è´Ÿé¢æç¤ºè¯: {negative_prompt}")
        print(f"  æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"  å¸§æ•°: {num_frames}")
        print(f"  å¸§ç‡: {fps}")
        print(f"  è§†é¢‘å°ºå¯¸: {width}x{height}")
        if seed is not None:
            print(f"  éšæœºç§å­: {seed}")
        print(f"{'='*60}\n")
        
        # ç”Ÿæˆè§†é¢‘
        print("æ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        print("è§†é¢‘ç”Ÿæˆæ¯”å›¾ç‰‡ç”Ÿæˆæ…¢å¾—å¤šï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        with torch.no_grad():
            result = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                num_frames=num_frames,
                height=height,
                width=width
            )
            
            # å®‰å…¨è·å–è§†é¢‘å¸§
            # output.frames çš„å¯èƒ½ç»“æ„ï¼š
            # 1. numpy.ndarray: shape=(1, num_frames, H, W, 3) - æœ€å¸¸è§
            # 2. List[List[Image]]: æ‰¹æ¬¡åˆ—è¡¨
            # 3. List[Image]: ç›´æ¥çš„å¸§åˆ—è¡¨
            
            import numpy as np
            
            if hasattr(result, 'frames'):
                frames = result.frames
                
                # æƒ…å†µ1: numpyæ•°ç»„æ ¼å¼
                if isinstance(frames, np.ndarray):
                    print(f"æ£€æµ‹åˆ°numpyæ•°ç»„æ ¼å¼ï¼Œshape: {frames.shape}")
                    if len(frames.shape) == 5:
                        frames_batch = frames[0]
                        from PIL import Image
                        video_frames = []
                        for i in range(frames_batch.shape[0]):
                            frame_data = frames_batch[i]
                            if frame_data.max() <= 1.0:
                                frame_data = (frame_data * 255).astype(np.uint8)
                            else:
                                frame_data = frame_data.astype(np.uint8)
                            img = Image.fromarray(frame_data)
                            video_frames.append(img)
                    else:
                        raise ValueError(f"æ„å¤–çš„numpyæ•°ç»„å½¢çŠ¶: {frames.shape}")
                
                # æƒ…å†µ2: åˆ—è¡¨æ ¼å¼
                elif isinstance(frames, list) and len(frames) > 0:
                    if isinstance(frames[0], list):
                        video_frames = frames[0]
                    else:
                        video_frames = frames
                
                else:
                    raise ValueError(f"æ¨¡å‹è¾“å‡ºçš„ frames æ ¼å¼ä¸æ”¯æŒï¼Œç±»å‹: {type(frames)}")
            
            elif hasattr(result, 'images'):
                video_frames = result.images
            
            else:
                raise ValueError("æ— æ³•ä»æ¨¡å‹è¾“å‡ºä¸­è·å–è§†é¢‘å¸§")
        
        # ä¿å­˜è§†é¢‘
        filepath = save_video(video_frames, output_name, "advanced_text_to_video", fps=fps)
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼")
        return video_frames, filepath


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä¸åŒå‚æ•°çš„æ•ˆæœ"""
    generator = AdvancedTextToVideo()
    
    # ç¤ºä¾‹1: åŸºç¡€ç”Ÿæˆ
    print("\nã€ç¤ºä¾‹1ã€‘åŸºç¡€ç”Ÿæˆ")
    generator.generate(
        prompt="a beautiful landscape with mountains and lake, peaceful, sunset",
        output_name="landscape_basic"
    )
    
    # ç¤ºä¾‹2: ä½¿ç”¨è´Ÿé¢æç¤ºè¯
    print("\nã€ç¤ºä¾‹2ã€‘ä½¿ç”¨è´Ÿé¢æç¤ºè¯")
    generator.generate(
        prompt="a cat playing with a ball, cute, cartoon style",
        negative_prompt="blurry, low quality, distorted, ugly",
        output_name="cat_negative"
    )
    
    # ç¤ºä¾‹3: æ›´å¤šå¸§æ•°ï¼ˆæ›´æµç•…ï¼‰
    print("\nã€ç¤ºä¾‹3ã€‘æ›´å¤šå¸§æ•°ï¼ˆæ›´æµç•…ï¼‰")
    generator.generate(
        prompt="ocean waves crashing on the beach, dynamic, cinematic",
        num_frames=24,
        fps=12,
        output_name="ocean_high_fps"
    )
    
    # ç¤ºä¾‹4: ä½¿ç”¨ç§å­å¤ç°ç»“æœ
    print("\nã€ç¤ºä¾‹4ã€‘ä½¿ç”¨å›ºå®šç§å­ï¼ˆå¯å¤ç°ï¼‰")
    seed = 42
    generator.generate(
        prompt="a robot walking in a futuristic city, cyberpunk style",
        seed=seed,
        output_name="robot_seed42"
    )


if __name__ == "__main__":
    main()

