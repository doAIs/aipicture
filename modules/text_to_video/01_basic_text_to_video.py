"""
åŸºç¡€ç¤ºä¾‹: æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼ˆæœ€ç®€å•ç‰ˆæœ¬ï¼‰
è¿™æ˜¯æœ€åŸºç¡€çš„æ–‡æœ¬ç”Ÿæˆè§†é¢‘ç¤ºä¾‹ï¼Œé€‚åˆåˆå­¦è€…ç†è§£åŸºæœ¬æµç¨‹
"""

from diffusers import DiffusionPipeline
import torch
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils.modules_utils import save_video, get_device, load_model_from_local_file, load_model_with_fallback
from config.modules_config import LOCAL_VIDEO_MODEL_PATH


def generate_video_from_text(prompt: str, output_name: str = None, num_frames: int = 16, fps: int = 8, local_model_path: str = None):
    """
    æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆè§†é¢‘
    
    Args:
        prompt: æ–‡æœ¬æè¿°ï¼Œä¾‹å¦‚ "a beautiful sunset over the ocean"
        output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        num_frames: è§†é¢‘å¸§æ•°ï¼ˆé»˜è®¤16å¸§ï¼‰
        fps: å¸§ç‡ï¼ˆé»˜è®¤8fpsï¼‰
        local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
                         - å¦‚æœä¸º Noneï¼Œåˆ™ä»é…ç½®æ–‡ä»¶ config.LOCAL_VIDEO_MODEL_PATH è¯»å–
                         - å¦‚æœä¸º "" æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                         - å¦‚æœæŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
    """
    print(f"\nå¼€å§‹ç”Ÿæˆè§†é¢‘...")
    print(f"æç¤ºè¯: {prompt}")
    print(f"å¸§æ•°: {num_frames}, å¸§ç‡: {fps}")
    
    # è·å–è®¾å¤‡
    device = get_device()
    
    # ç¡®å®šæœ¬åœ°æ¨¡å‹è·¯å¾„çš„ä¼˜å…ˆçº§
    if local_model_path is not None:
        model_path = local_model_path if local_model_path else None
    else:
        model_path = LOCAL_VIDEO_MODEL_PATH if LOCAL_VIDEO_MODEL_PATH else None
    
    # åŠ è½½æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼‰
    if model_path:
        print(f"\næœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
    else:
        print("\næœ¬åœ°æ¨¡å‹: å·²ç¦ç”¨ï¼ˆä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼‰")
    
    # æ ¹æ®è®¾å¤‡é€‰æ‹©æ•°æ®ç±»å‹
    if device == "cuda":
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32
    
    # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
    model_kwargs = {
        "torch_dtype": torch_dtype,
    }
    
    # ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆè§†é¢‘æ¨¡å‹
    # ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åœ¨çº¿æ¨¡å‹
    try:
        # ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹
        if model_path and os.path.exists(model_path):
            print(f"\nâœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
            print("   ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¦»çº¿æ¨¡å‹...")
            try:
                pipe = load_model_from_local_file(
                    DiffusionPipeline,
                    model_path,
                    **model_kwargs
                )
                pipe = pipe.to(device)
                print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"âš ï¸  æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"   å›é€€åˆ°åœ¨çº¿æ¨¡å‹: ali-vilab/text-to-video-ms-1.7b")
                # æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åœ¨çº¿æ¨¡å‹
                pipe = load_model_with_fallback(
                    DiffusionPipeline,
                    "ali-vilab/text-to-video-ms-1.7b",
                    **model_kwargs
                )
                pipe = pipe.to(device)
        elif model_path:
            # æœ¬åœ°æ¨¡å‹è·¯å¾„å·²é…ç½®ä½†ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
            print(f"\nâš ï¸  æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            print(f"   ä½¿ç”¨åœ¨çº¿æ¨¡å‹: ali-vilab/text-to-video-ms-1.7b")
            pipe = load_model_with_fallback(
                DiffusionPipeline,
                "ali-vilab/text-to-video-ms-1.7b",
                **model_kwargs
            )
            pipe = pipe.to(device)
        else:
            # æœ¬åœ°æ¨¡å‹æœªé…ç½®ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
            print(f"\nğŸ“¡ ä½¿ç”¨åœ¨çº¿æ¨¡å‹: ali-vilab/text-to-video-ms-1.7b")
            print("   æ³¨æ„ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹è¾ƒå¤§ï¼Œä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
            pipe = load_model_with_fallback(
                DiffusionPipeline,
                "ali-vilab/text-to-video-ms-1.7b",
                **model_kwargs
            )
            pipe = pipe.to(device)
        
        # ä¼˜åŒ–ï¼šå¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
        try:
            pipe.enable_attention_slicing()
            print("å·²å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ï¼ˆèŠ‚çœå†…å­˜ï¼‰")
        except:
            pass
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # ç”Ÿæˆè§†é¢‘
        print(f"\næ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        print("è§†é¢‘ç”Ÿæˆæ¯”å›¾ç‰‡ç”Ÿæˆæ…¢å¾—å¤šï¼Œè¯·è€å¿ƒç­‰å¾…...")

        with torch.no_grad():
            output = pipe(
                prompt,
                num_inference_steps=50,
                num_frames=num_frames
            )
            # .frames è¿”å›çš„æ˜¯ List[List[Image]]ï¼Œæˆ‘ä»¬éœ€è¦å–ç¬¬ä¸€ä¸ªè§†é¢‘ï¼ˆä¸‹æ ‡ [0]ï¼‰
            video_frames = output.frames[0]
        
        # ä¿å­˜è§†é¢‘
        filepath = save_video(video_frames, output_name, "basic_text_to_video", fps=fps)
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼")
        return video_frames, filepath
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\næç¤ºï¼š")
        print("1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼špip install -r requirements.txt")
        print("2. è§†é¢‘ç”Ÿæˆéœ€è¦å¤§é‡å†…å­˜ï¼Œå»ºè®®ä½¿ç”¨GPU")
        print("3. å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å°è¯•å‡å°‘num_frameså‚æ•°")
        raise


if __name__ == "__main__":
    # ç¤ºä¾‹: ç”Ÿæˆä¸€ä¸ªç®€å•çš„è§†é¢‘
    prompt = "a beautiful sunset over the ocean, peaceful, serene"
    generate_video_from_text(prompt, "sunset_ocean", num_frames=16, fps=8)

