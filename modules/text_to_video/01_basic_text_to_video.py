"""
åŸºç¡€ç¤ºä¾‹: æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼ˆæœ€ç®€å•ç‰ˆæœ¬ï¼‰
è¿™æ˜¯æœ€åŸºç¡€çš„æ–‡æœ¬ç”Ÿæˆè§†é¢‘ç¤ºä¾‹ï¼Œé€‚åˆåˆå­¦è€…ç†è§£åŸºæœ¬æµç¨‹
"""
from datetime import datetime

from diffusers import DiffusionPipeline
import torch
import sys
import os

from diffusers.utils import export_to_video

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils.modules_utils import save_video, get_device, load_model_from_local_file, load_model_with_fallback
from config.modules_config import LOCAL_VIDEO_MODEL_PATH, OUTPUT_VIDEOS_DIR


def generate_video_from_text(prompt: str, output_name: str = None, num_frames: int = 16, fps: int = 8, local_model_path: str = None):
    """
    è§†é¢‘æ—¶é•¿(ç§’) = num_frames Ã· fps
    æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆè§†é¢‘
    
    Args:
        prompt: æ–‡æœ¬æè¿°ï¼Œä¾‹å¦‚ "a beautiful sunset over the ocean"
        output_name: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        num_frames: è§†é¢‘å¸§æ•°ï¼ˆé»˜è®¤16å¸§ï¼‰
        ä½œç”¨ï¼šç”Ÿæˆçš„è§†é¢‘åŒ…å«å¤šå°‘å¸§å›¾åƒ
        é»˜è®¤å€¼ï¼š16 å¸§
         å½±å“ï¼šğŸ¬ å†³å®šè§†é¢‘ç´ æé‡ï¼šå¸§æ•°è¶Šå¤šï¼Œè§†é¢‘å†…å®¹è¶Šä¸°å¯Œ
         â±ï¸ å½±å“ç”Ÿæˆæ—¶é—´ï¼šå¸§æ•°è¶Šå¤šï¼Œç”Ÿæˆè¶Šæ…¢
         ğŸ’¾ å½±å“å†…å­˜å ç”¨ï¼šå¸§æ•°è¶Šå¤šï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜/å†…å­˜
         ğŸ“ ä¸æ—¶é•¿ç›¸å…³ï¼ˆè§ä¸‹æ–¹å…¬å¼ï¼‰

        fps: å¸§ç‡ï¼ˆé»˜è®¤8fpsï¼‰
        ä½œç”¨ï¼šFrames Per Secondï¼ˆæ¯ç§’æ’­æ”¾å¤šå°‘å¸§ï¼‰
             é»˜è®¤å€¼ï¼š8 fps
             å½±å“ï¼šğŸï¸ å†³å®šæ’­æ”¾é€Ÿåº¦ï¼šfps è¶Šé«˜ï¼Œè§†é¢‘è¶Šæµç•…
             ğŸ“ ä¸æ—¶é•¿ç›¸å…³ï¼ˆè§ä¸‹æ–¹å…¬å¼ï¼‰
             ğŸ¥ å¸¸è§æ ‡å‡†ï¼š8 fpsï¼šè¾ƒæ…¢ï¼ŒAIè§†é¢‘å¸¸ç”¨
                        24 fpsï¼šç”µå½±æ ‡å‡†
                        30 fpsï¼šè§†é¢‘æ ‡å‡†
                        60 fpsï¼šé«˜æ¸…æµç•…
        local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
                         - å¦‚æœä¸º Noneï¼Œåˆ™ä»é…ç½®æ–‡ä»¶ config.LOCAL_VIDEO_MODEL_PATH è¯»å–
                         - å¦‚æœä¸º "" æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                         - å¦‚æœæŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
    """
    print(f"\nå¼€å§‹ç”Ÿæˆè§†é¢‘...")
    print(f"æç¤ºè¯: {prompt}")
    print(f"å¸§æ•°: {num_frames}, å¸§ç‡: {fps}")
    
    # è·å–è®¾å¤‡
    device = get_device()
    
    # ç¡®å®šæœ¬åœ°æ¨¡å‹è·¯å¾„çš„ä¼˜å…ˆçº§
    if local_model_path is not None:
        model_path = local_model_path if local_model_path else None
    else:
        model_path = LOCAL_VIDEO_MODEL_PATH if LOCAL_VIDEO_MODEL_PATH else None
    
    # åŠ è½½æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼‰
    if model_path:
        print(f"\næœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
    else:
        print("\næœ¬åœ°æ¨¡å‹: å·²ç¦ç”¨ï¼ˆä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼‰")
    
    # æ ¹æ®è®¾å¤‡é€‰æ‹©æ•°æ®ç±»å‹
    if device == "cuda":
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32
    
    # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
    model_kwargs = {
        "torch_dtype": torch_dtype,
    }
    
    # ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆè§†é¢‘æ¨¡å‹
    # ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åœ¨çº¿æ¨¡å‹
    try:
        # ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹
        if model_path and os.path.exists(model_path):
            print(f"\nâœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
            print("   ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¦»çº¿æ¨¡å‹...")
            try:
                pipe = load_model_from_local_file(
                    DiffusionPipeline,
                    model_path,
                    **model_kwargs
                )
                pipe = pipe.to(device)
                print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            except Exception as e:
                print(f"âš ï¸  æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"   å›é€€åˆ°åœ¨çº¿æ¨¡å‹: ali-vilab/text-to-video-ms-1.7b")
                # æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åœ¨çº¿æ¨¡å‹
                pipe = load_model_with_fallback(
                    DiffusionPipeline,
                    "ali-vilab/text-to-video-ms-1.7b",
                    **model_kwargs
                )
                pipe = pipe.to(device)
        elif model_path:
            # æœ¬åœ°æ¨¡å‹è·¯å¾„å·²é…ç½®ä½†ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
            print(f"\nâš ï¸  æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            print(f"   ä½¿ç”¨åœ¨çº¿æ¨¡å‹: ali-vilab/text-to-video-ms-1.7b")
            pipe = load_model_with_fallback(
                DiffusionPipeline,
                "ali-vilab/text-to-video-ms-1.7b",
                **model_kwargs
            )
            pipe = pipe.to(device)
        else:
            # æœ¬åœ°æ¨¡å‹æœªé…ç½®ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
            print(f"\nğŸ“¡ ä½¿ç”¨åœ¨çº¿æ¨¡å‹: ali-vilab/text-to-video-ms-1.7b")
            print("   æ³¨æ„ï¼šè§†é¢‘ç”Ÿæˆæ¨¡å‹è¾ƒå¤§ï¼Œä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
            pipe = load_model_with_fallback(
                DiffusionPipeline,
                "ali-vilab/text-to-video-ms-1.7b",
                **model_kwargs
            )
            pipe = pipe.to(device)
        
        # ä¼˜åŒ–ï¼šå¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
        try:
            pipe.enable_attention_slicing()
            print("å·²å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ï¼ˆèŠ‚çœå†…å­˜ï¼‰")
        except:
            pass
        
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        
        # ç”Ÿæˆè§†é¢‘
        print(f"\næ­£åœ¨ç”Ÿæˆè§†é¢‘ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        print("è§†é¢‘ç”Ÿæˆæ¯”å›¾ç‰‡ç”Ÿæˆæ…¢å¾—å¤šï¼Œè¯·è€å¿ƒç­‰å¾…...")

        with torch.no_grad():
            output = pipe(
                prompt,
                num_inference_steps=50,
                num_frames=num_frames
            )

            ##ä½¿ç”¨å®˜æ–¹API
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"generated_{timestamp}"
            save_dir = os.path.join(OUTPUT_VIDEOS_DIR, "basic_text_to_video")
            filepath = os.path.join(save_dir, f"{filename}.mp4")
            os.makedirs(save_dir, exist_ok=True)
            video_path = export_to_video(video_frames=output.frames[0],output_video_path=filepath ,quality=5,fps=fps)
            print(f"================è§†é¢‘ä¿å­˜è·¯å¾„: {video_path}")
            
            # å®‰å…¨è·å–è§†é¢‘å¸§
            # output.frames çš„å¯èƒ½ç»“æ„ï¼š
            # 1. numpy.ndarray: shape=(1, num_frames, H, W, 3) - æœ€å¸¸è§
            # 2. List[List[Image]]: æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«ä¸€ä¸ªè§†é¢‘çš„å¸§åºåˆ—
            # 3. List[Image]: ç›´æ¥çš„å¸§åˆ—è¡¨
            
            import numpy as np
            
            if hasattr(output, 'frames'):
                frames = output.frames

                
                # æƒ…å†µ1: numpyæ•°ç»„æ ¼å¼ (shape: [batch, num_frames, H, W, C])
                if isinstance(frames, np.ndarray):
                    print(f"æ£€æµ‹åˆ°numpyæ•°ç»„æ ¼å¼ï¼Œshape: {frames.shape}")
                    # é€šå¸¸æ˜¯ (1, num_frames, height, width, 3)
                    if len(frames.shape) == 5:
                        # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡: [num_frames, H, W, C]
                        frames_batch = frames[0]
                        # è½¬æ¢ä¸ºPIL Imageåˆ—è¡¨
                        from PIL import Image
                        video_frames = []
                        for i in range(frames_batch.shape[0]):
                            frame_data = frames_batch[i]  # [H, W, C]
                            # ç¡®ä¿æ•°æ®åœ¨0-1èŒƒå›´å†…ï¼Œç„¶åè½¬æ¢ä¸º0-255
                            if frame_data.max() <= 1.0:
                                frame_data = (frame_data * 255).astype(np.uint8)
                            else:
                                frame_data = frame_data.astype(np.uint8)
                            # è½¬æ¢ä¸ºPIL Image
                            img = Image.fromarray(frame_data)
                            video_frames.append(img)
                    else:
                        raise ValueError(f"æ„å¤–çš„numpyæ•°ç»„å½¢çŠ¶: {frames.shape}ï¼ŒæœŸæœ›5ç»´æ•°ç»„ [batch, frames, H, W, C]")
                
                # æƒ…å†µ2: åˆ—è¡¨æ ¼å¼
                elif isinstance(frames, list) and len(frames) > 0:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºåµŒå¥—åˆ—è¡¨ï¼ˆæ‰¹æ¬¡ç»“æ„ï¼‰
                    if isinstance(frames[0], list):
                        video_frames = frames[0]  # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                    else:
                        video_frames = frames  # ç›´æ¥å°±æ˜¯å¸§åˆ—è¡¨
                
                else:
                    raise ValueError(f"æ¨¡å‹è¾“å‡ºçš„ frames æ ¼å¼ä¸æ”¯æŒï¼Œç±»å‹: {type(frames)}")
            
            elif hasattr(output, 'images'):
                # æŸäº›æ¨¡å‹å¯èƒ½ä½¿ç”¨ images å±æ€§
                video_frames = output.images
            
            else:
                raise ValueError("æ— æ³•ä»æ¨¡å‹è¾“å‡ºä¸­è·å–è§†é¢‘å¸§ï¼Œè¾“å‡ºç±»å‹: " + str(type(output)))
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å¸§æ•°æ®
        print(f"\nç”Ÿæˆäº† {len(video_frames)} å¸§")
        if len(video_frames) > 0:
            first_frame = video_frames[0]
            print(f"å¸§ç±»å‹: {type(first_frame)}")
            if hasattr(first_frame, 'size'):
                print(f"å¸§å°ºå¯¸: {first_frame.size}")
            if hasattr(first_frame, 'mode'):
                print(f"å¸§æ¨¡å¼: {first_frame.mode}")
            # è½¬æ¢ä¸ºnumpyæ£€æŸ¥æ•°æ®èŒƒå›´
            import numpy as np
            frame_array = np.array(first_frame)
            print(f"æ•°æ®ç±»å‹: {frame_array.dtype}")
            print(f"æ•°æ®èŒƒå›´: [{frame_array.min():.4f}, {frame_array.max():.4f}]")
            print(f"æ•°æ®å½¢çŠ¶: {frame_array.shape}")
        
        # ä¿å­˜è§†é¢‘
        filepath = save_video(video_frames, output_name, "basic_text_to_video", fps=fps)
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼")
        return video_frames, filepath
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\næç¤ºï¼š")
        print("1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼špip install -r requirements.txt")
        print("2. è§†é¢‘ç”Ÿæˆéœ€è¦å¤§é‡å†…å­˜ï¼Œå»ºè®®ä½¿ç”¨GPU")
        print("3. å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å°è¯•å‡å°‘num_frameså‚æ•°")
        raise


if __name__ == "__main__":
    # ç¤ºä¾‹: ç”Ÿæˆä¸€ä¸ªç®€å•çš„è§†é¢‘
    prompt = "a beautiful sunset over the ocean, peaceful, serene"
    generate_video_from_text(prompt, "sunset_ocean2", num_frames=16, fps=8)

