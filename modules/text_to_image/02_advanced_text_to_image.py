"""
è¿›é˜¶ç¤ºä¾‹ 3: æ–‡æœ¬ç”Ÿæˆå›¾ç‰‡ï¼ˆå¸¦å‚æ•°æ§åˆ¶ï¼‰
åŒ…å«æ›´å¤šå‚æ•°é€‰é¡¹ï¼Œå¯ä»¥ç²¾ç»†æ§åˆ¶ç”Ÿæˆæ•ˆæœ
"""

from diffusers import StableDiffusionPipeline
import torch
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils import save_image, get_device, set_seed
from config import (
    DEFAULT_STEPS,
    DEFAULT_GUIDANCE_SCALE,
    DEFAULT_HEIGHT,
    DEFAULT_WIDTH,
    DEFAULT_SEED,
    LOCAL_MODEL_PATH
)


class AdvancedTextToImage:
    """é«˜çº§æ–‡æœ¬ç”Ÿæˆå›¾ç‰‡ç±»"""
    
    def __init__(self, model_name: str = "stable-diffusion-v1-5/stable-diffusion-v1-5", local_model_path: str = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_name: åœ¨çº¿æ¨¡å‹åç§°ï¼ˆHuggingFace IDï¼‰
            local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
                            - å¦‚æœä¸º Noneï¼Œåˆ™ä»é…ç½®æ–‡ä»¶ config.LOCAL_MODEL_PATH è¯»å–
                            - å¦‚æœä¸º "" æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                            - å¦‚æœæŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
        """
        self.device = get_device()
        self.model_name = model_name
        
        # ç¡®å®šæœ¬åœ°æ¨¡å‹è·¯å¾„çš„ä¼˜å…ˆçº§ï¼š
        # 1. å¦‚æœä¼ å…¥äº† local_model_path å‚æ•°ï¼Œä½¿ç”¨ä¼ å…¥çš„å€¼ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºç¦ç”¨ï¼‰
        # 2. å¦åˆ™ä»é…ç½®æ–‡ä»¶è¯»å– LOCAL_MODEL_PATH
        if local_model_path is not None:
            self.local_model_path = local_model_path if local_model_path else None
        else:
            # ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœé…ç½®ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ç¦ç”¨æœ¬åœ°æ¨¡å‹
            self.local_model_path = LOCAL_MODEL_PATH if LOCAL_MODEL_PATH else None
        
        self.pipe = None
        print(f"åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼Œåœ¨çº¿æ¨¡å‹: {model_name}")
        if self.local_model_path:
            print(f"æœ¬åœ°æ¨¡å‹è·¯å¾„: {self.local_model_path}")
        else:
            print("æœ¬åœ°æ¨¡å‹: å·²ç¦ç”¨ï¼ˆä»…ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼‰")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œåªåœ¨éœ€è¦æ—¶åŠ è½½ï¼‰
        ä¼˜å…ˆåŠ è½½æœ¬åœ°ç¦»çº¿æ¨¡å‹ï¼Œå¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨åˆ™åŠ è½½åœ¨çº¿æ¨¡å‹
        """
        if self.pipe is None:
            from utils import load_model_from_local_file, load_model_with_fallback
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
                "safety_checker": None,
                "requires_safety_checker": False
            }
            
            # ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœå·²é…ç½®ï¼‰
            if self.local_model_path and os.path.exists(self.local_model_path):
                print(f"\nâœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {self.local_model_path}")
                print("   ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¦»çº¿æ¨¡å‹...")
                try:
                    self.pipe = load_model_from_local_file(
                        StableDiffusionPipeline,
                        self.local_model_path,
                        **model_kwargs
                    )
                    self.pipe = self.pipe.to(self.device)
                    print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                except Exception as e:
                    print(f"âš ï¸  æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    print(f"   å›é€€åˆ°åœ¨çº¿æ¨¡å‹: {self.model_name}")
                    # æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°åœ¨çº¿æ¨¡å‹
                    self.pipe = load_model_with_fallback(
                        StableDiffusionPipeline,
                        self.model_name,
                        **model_kwargs
                    )
                    self.pipe = self.pipe.to(self.device)
            elif self.local_model_path:
                # æœ¬åœ°æ¨¡å‹è·¯å¾„å·²é…ç½®ä½†ä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                print(f"\nâš ï¸  æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.local_model_path}")
                print(f"   ä½¿ç”¨åœ¨çº¿æ¨¡å‹: {self.model_name}")
                self.pipe = load_model_with_fallback(
                    StableDiffusionPipeline,
                    self.model_name,
                    **model_kwargs
                )
                self.pipe = self.pipe.to(self.device)
            else:
                # æœ¬åœ°æ¨¡å‹æœªé…ç½®ï¼Œç›´æ¥ä½¿ç”¨åœ¨çº¿æ¨¡å‹
                print(f"\nğŸ“¡ ä½¿ç”¨åœ¨çº¿æ¨¡å‹: {self.model_name}")
                self.pipe = load_model_with_fallback(
                    StableDiffusionPipeline,
                    self.model_name,
                    **model_kwargs
                )
                self.pipe = self.pipe.to(self.device)
            
            # ä¼˜åŒ–ï¼šå¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ï¼ˆå¦‚æœæ”¯æŒï¼‰
            try:
                self.pipe.enable_attention_slicing()
                print("å·²å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ï¼ˆèŠ‚çœå†…å­˜ï¼‰")
            except:
                pass
            
            print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    def generate(
        self,
        prompt: str,
        negative_prompt: str = None,
        num_inference_steps: int = DEFAULT_STEPS,
        guidance_scale: float = DEFAULT_GUIDANCE_SCALE,
        height: int = DEFAULT_HEIGHT,
        width: int = DEFAULT_WIDTH,
        seed: int = DEFAULT_SEED,
        output_name: str = None
    ):
        """
        ç”Ÿæˆå›¾ç‰‡
        
        Args:
            prompt: æ­£é¢æç¤ºè¯ï¼ˆæè¿°æƒ³è¦çš„å†…å®¹ï¼‰
            negative_prompt: è´Ÿé¢æç¤ºè¯ï¼ˆæè¿°ä¸æƒ³è¦çš„å†…å®¹ï¼‰
            num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆ20-100ï¼Œè¶Šå¤šè´¨é‡è¶Šå¥½ä½†è¶Šæ…¢ï¼‰
            guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆ1-20ï¼Œè¶Šé«˜è¶Šéµå¾ªæç¤ºè¯ï¼‰
            height: å›¾ç‰‡é«˜åº¦ï¼ˆå¿…é¡»æ˜¯8çš„å€æ•°ï¼‰
            width: å›¾ç‰‡å®½åº¦ï¼ˆå¿…é¡»æ˜¯8çš„å€æ•°ï¼‰
            seed: éšæœºç§å­ï¼ˆç”¨äºå¤ç°ç»“æœï¼‰
            output_name: è¾“å‡ºæ–‡ä»¶å
        
        Returns:
            ç”Ÿæˆçš„å›¾ç‰‡å’Œæ–‡ä»¶è·¯å¾„
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self.load_model()
        
        # è®¾ç½®éšæœºç§å­
        if seed is not None:
            set_seed(seed)
        
        print(f"\n{'='*60}")
        print(f"ç”Ÿæˆå‚æ•°:")
        print(f"  æç¤ºè¯: {prompt}")
        if negative_prompt:
            print(f"  è´Ÿé¢æç¤ºè¯: {negative_prompt}")
        print(f"  æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"  å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        print(f"  å›¾ç‰‡å°ºå¯¸: {width}x{height}")
        if seed is not None:
            print(f"  éšæœºç§å­: {seed}")
        print(f"{'='*60}\n")
        
        # ç”Ÿæˆå›¾ç‰‡
        print("æ­£åœ¨ç”Ÿæˆå›¾ç‰‡...")
        with torch.no_grad():
            result = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                height=height,
                width=width
            )
            image = result.images[0]
        
        # ä¿å­˜å›¾ç‰‡
        filepath = save_image(image, output_name, "advanced_text_to_image")
        print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼")
        return image, filepath


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä¸åŒå‚æ•°çš„æ•ˆæœ"""
    generator = AdvancedTextToImage()
    
    # ç¤ºä¾‹1: åŸºç¡€ç”Ÿæˆ
    print("\nã€ç¤ºä¾‹1ã€‘åŸºç¡€ç”Ÿæˆ")
    generator.generate(
        prompt="a majestic lion standing on a rock, sunset, photorealistic",
        output_name="lion_basic"
    )
    
    # ç¤ºä¾‹2: ä½¿ç”¨è´Ÿé¢æç¤ºè¯
    print("\nã€ç¤ºä¾‹2ã€‘ä½¿ç”¨è´Ÿé¢æç¤ºè¯ï¼ˆæ’é™¤ä¸æƒ³è¦çš„å†…å®¹ï¼‰")
    generator.generate(
        prompt="a beautiful landscape, mountains, lake, peaceful",
        negative_prompt="blurry, low quality, distorted, ugly",
        output_name="landscape_negative"
    )
    
    # ç¤ºä¾‹3: é«˜åˆ†è¾¨ç‡ç”Ÿæˆ
    print("\nã€ç¤ºä¾‹3ã€‘é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼ˆ768x768ï¼‰")
    generator.generate(
        prompt="a futuristic city at night, neon lights, cyberpunk style",
        height=768,
        width=768,
        num_inference_steps=60,  # é«˜åˆ†è¾¨ç‡éœ€è¦æ›´å¤šæ­¥æ•°
        output_name="city_highres"
    )
    
    # ç¤ºä¾‹4: ä½¿ç”¨ç§å­å¤ç°ç»“æœ
    print("\nã€ç¤ºä¾‹4ã€‘ä½¿ç”¨å›ºå®šç§å­ï¼ˆå¯å¤ç°ï¼‰")
    seed = 42
    generator.generate(
        prompt="a cute robot, cartoon style, colorful",
        seed=seed,
        output_name="robot_seed42"
    )
    
    # å†æ¬¡ä½¿ç”¨ç›¸åŒç§å­ï¼Œåº”è¯¥å¾—åˆ°ç›¸åŒç»“æœ
    generator.generate(
        prompt="a cute robot, cartoon style, colorful",
        seed=seed,
        output_name="robot_seed42_repeat"
    )


if __name__ == "__main__":
    main()

